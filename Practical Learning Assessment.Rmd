---
title: "Practical Machine Assignment"
author: "Tan Teck Hui"
output: html_document
---

Practical Machine Learning - An Analysis of the Weight Lifting Exercises Dataset
==================================================================================

# Executive Summary

One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal is to analyze data from accelerometers on the belt, forearm, arm, and dumbell of six participants. They were asked to perform barbell lifts correctly and incorrectly in five different ways.

For this assignment, the task is to analyse the  data provided to predict the manner in which each individual did their exercise.The libaries used in this assignment are caret and randomForest.


```{r}
library(Hmisc)
library(foreach)
library(doParallel)
library(caret)
library(randomForest)
set.seed(8888)
options(warn=-1)
```

# Data Processing and Analysis

Both training and test data are loaded. Values that does not conform to the rest of the dat  will be replaced by "NA" value. 

## Loading the datasets
```{r}
training_data <- read.csv("D:/Data Science Course/Module8_Assignment1/test/pml-training.csv", na.strings=c("#DIV/0!") )
testing_data <- read.csv("D:/Data Science Course/Module8_Assignment1/test/pml-testing.csv", na.strings=c("#DIV/0!") )
```

Converting data to numeric value 

```{r}
for(i in c(8:ncol(training_data)-1)) {training_data[,i] = as.numeric(as.character(training_data[,i]))}

for(i in c(8:ncol(testing_data)-1)) {testing_data[,i] = as.numeric(as.character(testing_data[,i]))}
```

## Missing Data & Feature filtering

Missing values are moved from the data set to reduce inaccuracy of the prediction. In order to improve accuracy, the following columns are removed as well : username, timestamps, windows. 


```{r}
feature_set <- colnames(training_data[colSums(is.na(training_data)) == 0])[-(1:7)]
model_data <- training_data[feature_set]
feature_set
```

# Prediction model building

For training the model, the data will be split into training and validation sets. 

```{r}
idx <- createDataPartition(y=model_data$classe, p=0.75, list=FALSE )
training <- model_data[idx,]
testing <- model_data[-idx,]
```

The random forest algorithm will be used for this prediction. 

```{r}
registerDoParallel()
x <- training[-ncol(training)]
y <- training$classe

rf <- foreach(ntree=rep(150, 6), .combine=randomForest::combine, .packages='randomForest') %dopar% {
randomForest(x, y, ntree=ntree) 
}
```

# Results and model accuracy

The following shows the training and testing accuracy based on the model that was built.

```{r}
predictions1 <- predict(rf, newdata=training)
confusionMatrix(predictions1,training$classe)


predictions2 <- predict(rf, newdata=testing)
confusionMatrix(predictions2,testing$classe)
```

# Conclusion
The random Forest algorithm is accurate against the model that was built, with a 99% testing accuracy

# Testing Set for submission

Using the generated model on the testing set provided.

```{r}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}


x <- testing_data
x <- x[feature_set[feature_set!='classe']]
answers <- predict(rf, newdata=x)

answers

pml_write_files(answers)
```
